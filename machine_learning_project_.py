# -*- coding: utf-8 -*-
"""Machine Learning - Project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1arZF_3oSKOoUT-Lsz-eTqzHxPdnRRHkc
"""

import pandas as pd

df=pd.read_csv("/content/BankChurners.csv")
df

df.columns

df

df.columns

df = df.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'])

df.columns

df

df.tail()

df.shape

import pandas as pd

# Assuming df is your DataFrame
df = df.drop(columns=['Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1'])

# Display the DataFrame after dropping the column
print(df)

df.info()

df.describe(include="all")

df[5:]

df.dtypes

df.head()

"""**histogram**"""

df.columns

import matplotlib.pyplot as plt
import seaborn as sns
plt.hist(df['Credit_Limit'],bins=100)

"""* It is a right skewed data"""

plt.hist(df['Customer_Age'],bins=100)

plt.hist(df['Months_on_book'],bins=100)

plt.hist(df['Total_Revolving_Bal'],bins=100)

plt.hist(df['Avg_Open_To_Buy'],bins=100)

"""* It is a right skewed Data"""

plt.hist(df['Total_Amt_Chng_Q4_Q1'],bins=100)

plt.hist(df['Total_Trans_Amt'],bins=200)

"""* It is also a multimodel data"""

plt.hist(df['Total_Trans_Ct'],bins=100)

"""* It is a multimodal data"""

plt.hist(df['Total_Ct_Chng_Q4_Q1'],bins=100)

plt.hist(df['Avg_Utilization_Ratio'],bins=100)

"""Scatter plot"""

plt.scatter(df['Customer_Age'],df['Credit_Limit'],alpha=0.5)

plt.scatter(df['Months_on_book'],df['Credit_Limit'],alpha=0.5)

plt.scatter(df['Customer_Age'],df['Avg_Open_To_Buy'],alpha=0.5)

plt.scatter(df['Customer_Age'],df['Total_Amt_Chng_Q4_Q1'],alpha=0.5)

plt.scatter(df['Customer_Age'],df['Total_Trans_Amt'],alpha=0.5)

plt.scatter(df['Customer_Age'],df['Total_Ct_Chng_Q4_Q1'],alpha=0.5)

plt.scatter(df['Customer_Age'],df['Total_Trans_Ct'],alpha=0.5)

plt.scatter(df['Customer_Age'],df['Avg_Utilization_Ratio'],alpha=0.5)

plt.scatter(df['Total_Amt_Chng_Q4_Q1'],df['Total_Ct_Chng_Q4_Q1'],alpha=0.5)

"""piechart

"""

plt.pie(df.Gender.value_counts(). values, labels=df.Gender.value_counts().index,autopct='%.2f%%')
plt.legend()

Existing_Customers=df[(df['Attrition_Flag'] == 'Existing Customer')]
Attrited_Customers=df[(df['Attrition_Flag'] == 'Attrited Customer')]


plt.pie(Attrited_Customers.Gender.value_counts(). values, labels=Attrited_Customers.Gender.value_counts().index,autopct='%.2f%%')
plt.legend()
plt.title('Attrited Customer Gender Distribution')

Existing_Customers=df[(df['Attrition_Flag'] == 'Existing Customer')]
Attrited_Customers=df[(df['Attrition_Flag'] == 'Attrited Customer')]


plt.pie(Attrited_Customers.Marital_Status.value_counts(). values, labels=Attrited_Customers.Marital_Status.value_counts().index,autopct='%.2f%%')
plt.legend()
plt.title('Attrited Customer Marital Status Distribution')

Existing_Customers=df[(df['Attrition_Flag'] == 'Existing Customer')]
Attrited_Customers=df[(df['Attrition_Flag'] == 'Attrited Customer')]


plt.pie(Attrited_Customers.Education_Level.value_counts(). values, labels=Attrited_Customers.Education_Level.value_counts().index,autopct='%.2f%%')
plt.legend()
plt.title('Attrited Customer Education Level Distribution')

Existing_Customers=df[(df['Attrition_Flag'] == 'Existing Customer')]
Attrited_Customers=df[(df['Attrition_Flag'] == 'Attrited Customer')]


plt.pie(Attrited_Customers.Income_Category.value_counts(). values, labels=Attrited_Customers.Income_Category.value_counts().index,autopct='%.2f%%')
plt.legend()
plt.title('Attrited Customer Income Level Distribution')

Existing_Customers=df[(df['Attrition_Flag'] == 'Existing Customer')]
Attrited_Customers=df[(df['Attrition_Flag'] == 'Attrited Customer')]


plt.pie(Attrited_Customers.Dependent_count.value_counts(). values, labels=Attrited_Customers.Dependent_count.value_counts().index,autopct='%.2f%%')
plt.legend()
plt.title('Attrited Customer Dependent_Count Distribution')

Existing_Customers_Female=df[(df['Attrition_Flag'] == 'Existing Customer') & (df['Gender']=='F')]
Attrited_Customers_Female=df[(df['Attrition_Flag'] == 'Attrited Customer') & (df['Gender']=='F')]


plt.pie(Attrited_Customers_Female.Card_Category.value_counts(). values, labels=Attrited_Customers.Card_Category.value_counts().index,autopct='%.2f%%')
plt.legend()
plt.title('Female Attrited Customer Card Category Distribution')

"""## Female Customers who have attrited are mostly blue card holders

"""

plt.pie(Attrited_Customers.Card_Category.value_counts(). values, labels=Attrited_Customers.Card_Category.value_counts().index,autopct='%.2f%%')
plt.legend()
plt.title('Attrited Customer Card Type Distribution')

"""## The customers who have attrited are mostly Blue card holders."""

Existing_Customers=df[(df['Attrition_Flag'] == 'Existing Customer')]
Attrited_Customers=df[(df['Attrition_Flag'] == 'Attrited Customer')]


plt.pie(Attrited_Customers.Months_Inactive_12_mon.value_counts(). values, labels=Attrited_Customers.Months_Inactive_12_mon.value_counts().index,autopct='%.2f%%')
plt.legend()
plt.title('Attrited Customer Inactive month count Distribution')

plt.pie(df.Attrition_Flag.value_counts(). values, labels=df.Attrition_Flag.value_counts().index,autopct='%.2f%%')
plt.legend()

plt.pie(df.Dependent_count.value_counts(). values, labels=df.Dependent_count.value_counts().index,autopct='%.2f%%')
plt.legend()

plt.pie(df.Education_Level.value_counts(). values, labels=df.Education_Level.value_counts().index,autopct='%.2f%%')
plt.legend()

plt.pie(df.Marital_Status.value_counts(). values, labels=df.Marital_Status.value_counts().index,autopct='%.2f%%')
plt.legend()

plt.pie(df.Income_Category.value_counts(). values, labels=df.Income_Category.value_counts().index,autopct='%.2f%%')
plt.legend()

plt.pie(df.Card_Category.value_counts(). values, labels=df.Card_Category.value_counts().index,autopct='%.2f%%')
plt.legend()

plt.pie(df.Months_Inactive_12_mon.value_counts(). values, labels=df.Months_Inactive_12_mon.value_counts().index,autopct='%.2f%%')
plt.legend()

plt.pie(df.Contacts_Count_12_mon.value_counts(). values, labels=df.Contacts_Count_12_mon.value_counts().index,autopct='%.2f%%')
plt.legend()



"""count plot"""

import seaborn as sns
sns.countplot(x='Attrition_Flag', data=df)

sns.countplot(x='Gender', data=df)

sns.countplot(x='Education_Level', data=df)

sns.countplot(x='Marital_Status', data=df)

sns.countplot(x='Income_Category', data=df)

sns.countplot(x='Card_Category', data=df)

sns.countplot(x='Months_Inactive_12_mon', data=df)

sns.countplot(x='Contacts_Count_12_mon', data=df)

"""BOXPLOT"""

sns.boxplot(x='Customer_Age', data=df)

print(df['Customer_Age'].quantile(0.99))

import numpy as np
df['Customer_Age'] = np.where(df['Customer_Age'] >65.0 , 65.0, df['Customer_Age'])

sns.boxplot(x='Customer_Age', data=df)

sns.boxplot(x='Months_on_book', data=df)

print(df['Months_on_book'].quantile(0.05))
print(df['Months_on_book'].quantile(0.95))

df['Months_on_book'] = np.where(df['Months_on_book'] > 50.0, 50.0, df['Months_on_book'])
df['Months_on_book'] = np.where(df['Months_on_book'] < 22.0, 22.0, df['Months_on_book'])

sns.boxplot(x='Months_on_book', data=df)

sns.boxplot(x='Months_on_book', data=df)

sns.boxplot(x='Credit_Limit', data=df)

plt.hist(df['Avg_Utilization_Ratio'],bins=100)

print(df['Credit_Limit'].quantile(0.90))

df['Credit_Limit'] = np.where(df['Credit_Limit'] > 23400.199999999997, 23400.199999999997, df['Credit_Limit'])

sns.boxplot(x='Credit_Limit', data=df)

sns.boxplot(x='Total_Revolving_Bal', data=df)

sns.boxplot(x='Avg_Open_To_Buy', data=df)

print(df['Avg_Open_To_Buy'].quantile(0.90))

df['Avg_Open_To_Buy'] = np.where(df['Avg_Open_To_Buy'] > 21964.6, 21964.6, df['Avg_Open_To_Buy'])

sns.boxplot(x='Avg_Open_To_Buy', data=df)

sns.boxplot(x='Total_Amt_Chng_Q4_Q1', data=df)

print(df['Total_Amt_Chng_Q4_Q1'].quantile(0.05))
print(df['Total_Amt_Chng_Q4_Q1'].quantile(0.93))

df['Total_Amt_Chng_Q4_Q1'] = np.where(df['Total_Amt_Chng_Q4_Q1'] > 1.045, 1.045, df['Total_Amt_Chng_Q4_Q1'])
df['Total_Amt_Chng_Q4_Q1'] = np.where(df['Total_Amt_Chng_Q4_Q1'] < 0.463, 0.463, df['Total_Amt_Chng_Q4_Q1'])

sns.boxplot(x='Total_Amt_Chng_Q4_Q1', data=df)

sns.boxplot(x='Total_Trans_Amt', data=df)

print(df['Total_Trans_Amt'].quantile(0.90))

df['Total_Trans_Amt'] = np.where(df['Total_Trans_Amt'] > 8212.4, 8212.4, df['Total_Trans_Amt'])

sns.boxplot(x='Total_Trans_Amt', data=df)

sns.boxplot(x='Total_Trans_Ct', data=df)

print(df['Total_Trans_Ct'].quantile(0.97))

df['Total_Trans_Ct'] = np.where(df['Total_Trans_Ct'] > 114.0, 114.0, df['Total_Trans_Ct'])

sns.boxplot(x='Total_Trans_Ct', data=df)

sns.boxplot(x='Total_Ct_Chng_Q4_Q1', data=df)

print(df['Total_Ct_Chng_Q4_Q1'].quantile(0.05))
print(df['Total_Ct_Chng_Q4_Q1'].quantile(0.93))

df['Total_Ct_Chng_Q4_Q1'] = np.where(df['Total_Ct_Chng_Q4_Q1'] > 1.0,1.0, df['Total_Ct_Chng_Q4_Q1'])
df['Total_Ct_Chng_Q4_Q1'] = np.where(df['Total_Ct_Chng_Q4_Q1'] < 0.368, 0.368, df['Total_Ct_Chng_Q4_Q1'])

sns.boxplot(x='Total_Ct_Chng_Q4_Q1', data=df)

sns.boxplot(x='Avg_Utilization_Ratio', data=df)

import seaborn as sns
import matplotlib.pyplot as plt

# Assuming df is your DataFrame and seaborn has been imported as sns

# Set the figure size
plt.figure(figsize=(12, 10))

# Create the heatmap with larger cell size
sns.heatmap(df.corr(), annot=True, annot_kws={'size': 12}, cmap="RdBu", square=True)

# Rotate tick labels if needed
plt.xticks(rotation=45)
plt.yticks(rotation=45)

# Add title
plt.title("Correlation Heatmap", fontsize=16)

# Show plot
plt.tight_layout()
plt.show()



"""__HANDLING MISSING VALUES(5 ways): 1 Drop missing values(row/column) 2 Replace missing values with- average, frequency of elements 3 Replacing with the previous value – forward fill .fillna(method=‘ffill') 4 Replacing with the next value – backward fill .fillna(method=‘bfill') 5 Interpolation:- Pandas’ interpolate method can be used to replace the missing values with different interpolation methods like ‘polynomial,’ ‘linear,’ and ‘quadratic.’ The default method is ‘linear.’.interpolate()"""

df.isna().sum()

df['Attrition_Flag'].value_counts()

df['Attrition_Flag'] = df['Attrition_Flag'].map({'Existing Customer':0, 'Attrited Customer':1})

"""## Aplying one hot encoding"""

dummy_df = pd.get_dummies(df['Card_Category'])
df = pd.concat([df, dummy_df], axis=1)

dummy_df = pd.get_dummies(df['Marital_Status'])
df = pd.concat([df, dummy_df], axis=1)

dummy_df = pd.get_dummies(df['Income_Category'])
df = pd.concat([df, dummy_df], axis=1)

dummy_df = pd.get_dummies(df['Education_Level'])
df = pd.concat([df, dummy_df], axis=1)

df

df['Attrition_Flag'].value_counts()

df['Gender'].value_counts()

df['Gender'] = df['Gender'].map({'M':0, 'F':1})

df['Gender'].value_counts()

df['Education_Level'].value_counts()

df['Education_Level'] = df['Education_Level'].map({'Graduate':0, 'High School':1, 'Unknown':2, 'Uneducated':3, 'College':4, 'Post-Graduate':5, 'Doctorate':6})

df['Education_Level'].value_counts()

df['Marital_Status'].value_counts()

df['Marital_Status'] = df['Marital_Status'].map({'Married':0, 'Single':1, 'Unknown':2, 'Divorced':3})

df['Marital_Status'].value_counts()

df['Income_Category'].value_counts()

df['Income_Category'] = df['Income_Category'].map({'Less than $40K':0, '$40K - $60K':1, '$80K - $120K':2, '$60K - $80K':3, 'Unknown':4, '$120K +':5})

df['Income_Category'].value_counts()

df['Card_Category'].value_counts()

df['Card_Category'] = df['Card_Category'].map({'Blue':0, 'Silver':1, 'Gold':2, 'Platinum':3})

df['Card_Category'].value_counts()

df.info()

df

df.drop('Unknown',axis=1)

"""## Applying SMOTE(SYNTHETIC MINORITY OVERSAMPLING TECHNIQUE TO UPSAMPLE THE MINORITY ELEMENT):
  This is basically a synthetic oversampling technique which is basically used for preventing the minority classes from getting overfitted.It takes place by the extrapolation of the data  
"""

from imblearn.over_sampling import SMOTE

oversample = SMOTE()
X,y =oversample.fit_resample(df[df.columns[1:]],df[df.columns[1]])
upsampled_df=X.assign(churn=y)

"""## So here we baasically upsampled the data"""

upsampled_df

upsampled_df.drop('Unknown',axis=1)

"""## So as we can see that upsampling this data resulted in the increase in the number of observation and it lead to the increase in the balannce of the data in different classes

## Applying Principal Component Analysis (PCA):
Principal Component Analysis (PCA) is a dimensionality reduction technique used to simplify and streamline the complexity of high-dimensional data while retaining most of the variability present in the data.\
Reasons of Using PCA:-
1. Dimensionality Reduction: PCA helps in reducing the number of features (or dimensions) in a dataset while preserving most of the important information.
2. Data Visualization: PCA can be used to visualize high-dimensional data in lower-dimensional space
3. Noise Reduction: PCA can help in removing noise and redundancy in the data by capturing the underlying structure or patterns\
So here we are using this PCA since if there is some dependencies among the features we would not be considering that.
Note: We are just applying the pca on the numerical data
"""

from sklearn.decomposition import PCA

upsampled_df.info()

n_components = 4
pca = PCA(n_components=n_components)
upsampled_df_numerical = upsampled_df[['Avg_Open_To_Buy','Credit_Limit','Total_Revolving_Bal','Total_A  mt_Chng_Q4_Q1','Total_Trans_Amt','Total_Trans_Ct','Total_Ct_Chng_Q4_Q1']]
X_pca = pca.fit_transform(upsampled_df_numerical)

X_pca

columns = [f"PC{i+1}" for i in range(n_components)]
X_pca_df = pd.DataFrame(data=X_pca, columns=columns)

X_pca_df

upsampleddf_with_pca = pd.concat([upsampled_df, X_pca_df], axis=1)

upsampleddf_with_pca

upsampleddf_with_pca.drop(['Unknown'],axis=1)

"""## Applying the models and Evaluation

#Importing Necessary Files
"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier

from sklearn.ensemble import AdaBoostClassifier
from sklearn.svm import SVC

"""## Applying Random Forest Classifier"""

rf_pipe =Pipeline(steps=[('scale',StandardScaler()),("RF",RandomForestClassifier(random_state=42))])

"""## Applying AdaBoost Classifier"""

ada_pipe =Pipeline(steps=[('scale',StandardScaler()),("RF",AdaBoostClassifier(random_state=42,learning_rate=0.7))])

"""## Applying Support Vector Machine Classifier"""

svm_pipe =Pipeline(steps=[('scale',StandardScaler()),("RF",SVC(random_state=42,kernel='rbf'))])

"""## Split the dataset in test and train data"""

from sklearn.model_selection import train_test_split

X = upsampleddf_with_pca.loc[:, ['PC1','PC2','PC3','PC4','Total_Trans_Ct','Total_Ct_Chng_Q4_Q1','Total_Relationship_Count']]  # All rows and all columns starting from the second colum
y = upsampleddf_with_pca.iloc[:, :1]

X

y

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=42)

"""## Testing of the Model"""

from sklearn.model_selection import cross_val_score

"""## Cross Validation Techniques"""

rf_Score= cross_val_score(rf_pipe,X_train,y_train,cv=5,scoring='f1')

ada_Score= cross_val_score(ada_pipe,X_train,y_train,cv=5,scoring='f1')

svm_Score= cross_val_score(svm_pipe,X_train,y_train,cv=5,scoring='f1')

rf_Score

ada_Score

svm_Score

rf_pipe.fit(X_train,y_train)

svm_pipe.fit(X_train,y_train)

ada_pipe.fit(X_train,y_train)

from sklearn.metrics import accuracy_score

predictions = rf_pipe.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(accuracy)

predictions = ada_pipe.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(accuracy)

predictions = svm_pipe.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(accuracy)

"""# Applying Hyperparameter Tuning"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'RF__n_estimators': [50, 100, 200],
    'RF__max_depth': [None, 5, 10],
    'RF__min_samples_split': [2, 5, 10]
}

grid_search = GridSearchCV(rf_pipe, param_grid, cv=5, scoring='accuracy')
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best accuracy:", grid_search.best_score_)

import pickle
filename = 'randomForest.pkl'

# Save the model to disk
with open(filename, 'wb') as file:
    pickle.dump(grid_search, file)

import pickle
filename = 'SVM.pkl'

# Save the model to disk
with open(filename, 'wb') as file:
    pickle.dump(svm_pipe, file)

import pickle
filename = 'adaBoost.pkl'

# Save the model to disk
with open(filename, 'wb') as file:
    pickle.dump(ada_pipe, file)